--------------------------------------------------------------------------------------------------------------------
{Day 0}

Environment SetUp

Versions
java 1.8.0_191
Apache Spark - 2.1.1
Scala - 2.11.11
Cassandra - 3.0.17
IntelliJ IDEA 2018.3.3 (Community Edition)
DevCenter Datastax 1.5.0




--------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
{Day 1}




(1) Word Count program in spark scala,using Intellij IDE

Used dependencies - spark-core_2.11,spark-sql_2.11

Set Logger level to error to avoid unnecessary info messages on console
Created a saprkSession followed by sparkcontext
Read data from List and used SparkContext.Parallelize to create rdd,
Count using map and CountByValue
printed result into Console using print()

--------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
{Day 2}

(2) Create a Table in Cassandra using Datastax and insert values into it

create,insert,select operations

(3) Connect cassandra with spark and show table details in spark console

Used Dependencies - spark-cassandra-connector-unshaded_2.11,cassandra-driver-core,commons-codec

Create rdd using SparkContext.cassandraTable

(4) Write into casssandra Table

Parallelize data and rdd.saveToCassandra to store data into cassandra table.
--------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
{Day 3}

(5) Read csv file with specifying schema to it and create dataframe.Then save that Df into specified file path with specified name as csv with headers.

using coalesce(1), we can create partitions files into one file.
used sparkContext.hadoopConfiguration to create hdfs instance .
using hdfs filepath,saved into the path and rename into specified name. 
refer : https://www.programcreek.com/scala/org.apache.hadoop.fs.FileSystem
--------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
{Day 4}
--------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
{Day 5}
--------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
{Day 6}
--------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
{Day 7}
--------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
{Day 8}
--------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
{Day 9}
--------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
{Day 10}
--------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
{Day 11}
--------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
{Day 12}
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
{Day 4}
--------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
{Day 4}
--------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
{Day 4}
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
{Day 4}
--------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------
{Day 4}
--------------------------------------------------------------------------------------------------------------------
